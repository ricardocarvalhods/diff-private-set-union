{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "238cc94c-2ec3-4e19-90c1-c9a66ce14f22",
   "metadata": {},
   "source": [
    "# Note\n",
    "\n",
    "- In the ZIP file in this submission we already include a cleaned version of the sensitive dataset 'finance' and public dataset 'imdb'.\n",
    "- Therefore, you only need to execute this notebook if you want to download and preprocess other datasets.\n",
    "- On the other hand, if you want to run the algorithms only using the datasets provided in the submission file ('finance' and 'imdb'), please skip this notebook and go straight to `2_run_dpsu.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebdc7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723c6d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import wget\n",
    "import pandas as pd\n",
    "import email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c69cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0ae019",
   "metadata": {},
   "source": [
    "**Pre-processing function from Gopi et. al 2020**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7be9fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reddit_preprocess(text): \n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\n', ' ', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\[removed\\]', ' ', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\[deleted\\]', ' ', text, flags=re.MULTILINE)\n",
    "    sentences = nltk.tokenize.sent_tokenize(text)\n",
    "    sentences = [\" \".join(nltk.tokenize.word_tokenize(s)) for s in sentences] \n",
    "    return \" \".join(sentences) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a2bc80",
   "metadata": {},
   "source": [
    "# Reddit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc9a816-2d78-4828-9c9f-3efb0652c21c",
   "metadata": {},
   "source": [
    "The Reddit dataset was gathered by [Gopi et. al 2020](https://arxiv.org/abs/2002.09745) and made available in their [github repository](https://github.com/heyyjudes/differentially-private-set-union).\n",
    "\n",
    "To download the dataset, please go to [this link](https://github.com/heyyjudes/differentially-private-set-union/blob/ea7b39285dace35cc9e9029692802759f3e1c8e8/data/clean_askreddit.csv.zip) and download the file `clean_askreddit.csv.zip` by clicking on the button **Download**.\n",
    "\n",
    "After that, unzip the content and put the file `clean_askreddit.csv` in this project's `data` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b12ba4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Gopi et. al 2020) already shares the cleaned dataset.\n",
    "# Therefore, we do not have to clean it again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4876539f",
   "metadata": {},
   "source": [
    "# Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0484a6c8-6df8-42c7-9362-200d6d79e495",
   "metadata": {},
   "source": [
    "The Twitter dataset was made available in the Kaggle platform.\n",
    "\n",
    "To download the dataset, please go to [this link](https://www.kaggle.com/thoughtvector/customer-support-on-twitter) and click on the button **Download** in the upper right corner.\n",
    "\n",
    "After that, unzip the content and put the file `twcs/twcs.csv` in this project's `data` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1836dc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter = pd.read_csv('data/twcs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803e37bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter = df_twitter[['author_id', 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422fc85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter['clean_text'] = df_twitter['text'].apply(lambda x: reddit_preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0b2b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter = df_twitter[['author_id', 'clean_text']]\n",
    "df_twitter.columns = ['author', 'clean_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea61f9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned .csv\n",
    "df_twitter.to_csv('data/twitter_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56de314",
   "metadata": {},
   "source": [
    "# Finance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb646a11-7be8-45be-9f87-670febfd1147",
   "metadata": {},
   "source": [
    "The Finance dataset was made available in the Kaggle platform.\n",
    "\n",
    "To download the dataset, please go to [this link](https://www.kaggle.com/miguelaenlle/massive-stock-news-analysis-db-for-nlpbacktests). In the data explorer, choose `analyst_ratings_processed.csv` and then click on the symbol with a arrow pointing down to download just this file.\n",
    "\n",
    "After that, unzip the content and put the file `analyst_ratings_processed.csv` in this project's `data` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e70937-8989-4880-9208-ecfc1791bb79",
   "metadata": {},
   "source": [
    "**PLEASE NOTE THAT WE ARE ALREADY SUBMITTING THE CLEANED VERSION OF THIS DATASET AT `data\\finance_cleaned.csv`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8842e7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_finance = pd.read_csv('data/analyst_ratings_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1038bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_finance = df_finance[['title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439b3b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_finance['author'] = range(df_finance.shape[0])\n",
    "df_finance['author'] = df_finance['author'].apply(lambda x: 'a' + str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0872f311",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_finance['clean_text'] = df_finance['title'].apply(lambda x: reddit_preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30a1b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_finance.drop(['title'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cad7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned .csv\n",
    "df_finance.to_csv('data/finance_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abcd3ed",
   "metadata": {},
   "source": [
    "# IMDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21f8adf-ed02-45e8-a722-b9296fc1eb17",
   "metadata": {},
   "source": [
    "The IMDB dataset was made available in the Kaggle platform.\n",
    "\n",
    "To download the dataset, please go to [this link](https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews) and click on the button **Download** in the upper right corner.\n",
    "\n",
    "After that, unzip the content and put the file `IMDB Dataset.csv` in this project's `data` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4ee227-ed80-4a26-97cd-54ea0b8cb86d",
   "metadata": {},
   "source": [
    "**PLEASE NOTE THAT WE ARE ALREADY SUBMITTING THE CLEANED VERSION OF THIS DATASET AT `data\\imdb_cleaned.csv`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e7049d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imdb = pd.read_csv('data/IMDB Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7e209c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imdb['author'] = range(df_imdb.shape[0])\n",
    "df_imdb['author'] = df_imdb['author'].apply(lambda x: 'a' + str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d93a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imdb['clean_text'] = df_imdb['review'].apply(lambda x: reddit_preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa384f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imdb.drop(['review', 'sentiment'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea48d5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned .csv\n",
    "df_imdb.to_csv('data/imdb_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ccf7fa",
   "metadata": {},
   "source": [
    "# Covid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35eca739-11a4-4ca6-9a4a-5b50273317c4",
   "metadata": {},
   "source": [
    "The Covid dataset was made available in the Kaggle platform.\n",
    "\n",
    "To download the dataset, please go to [this link](https://www.kaggle.com/fmitchell259/covid19-medical-paperscsv) and click on the button **Download** in the upper right corner.\n",
    "\n",
    "After that, unzip the content and put the file `kaggle_covid-19_open_csv_format.csv` in this project's `data` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6de4089",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid = pd.read_csv('data/kaggle_covid-19_open_csv_format.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9656f705",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid = df_covid[['text_body']].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bafd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid['author'] = range(df_covid.shape[0])\n",
    "df_covid['author'] = df_covid['author'].apply(lambda x: 'a' + str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdec01f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid['clean_text'] = df_covid['text_body'].apply(lambda x: reddit_preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dbae94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid.drop(['text_body'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5dbd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned .csv\n",
    "df_covid.to_csv('data/covid_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f6de83",
   "metadata": {},
   "source": [
    "# Songs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f02e60-dbe2-4c79-a92b-ca8161ddc94a",
   "metadata": {},
   "source": [
    "The Songs dataset was made available in the Kaggle platform.\n",
    "\n",
    "To download the dataset, please go to [this link](https://www.kaggle.com/edenbd/150k-lyrics-labeled-with-spotify-valence) and click on the button **Download** in the upper right corner.\n",
    "\n",
    "After that, unzip the content and put the file `labeled_lyrics_cleaned.csv` in this project's `data` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b583fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_songs = pd.read_csv('data/labeled_lyrics_cleaned.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8aac76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_songs['author'] = df_songs['artist']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6a7d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_songs['clean_text'] = df_songs['seq'].apply(lambda x: reddit_preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f507d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_songs.drop(['artist', 'seq', 'song', 'label'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e7d23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned .csv\n",
    "df_songs.to_csv('data/songs_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665d1d1b",
   "metadata": {},
   "source": [
    "# Wiki"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1514849d-002e-40fd-84bc-8f63b8049815",
   "metadata": {},
   "source": [
    "The Wiki dataset was made available in the Kaggle platform.\n",
    "\n",
    "To download the dataset, please go to [this link](https://www.kaggle.com/markwijkhuizen/simplenormal-wikipedia-abstracts-v1) and click on the button **Download** in the upper right corner.\n",
    "\n",
    "After that, unzip the content and put the file `wikipedia_abstracts.pkl` in this project's `data` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2896650d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wiki = pd.read_pickle('data/wikipedia_abstracts.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7c905d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wiki = df_wiki[['abstract_original']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ee76ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wiki = df_wiki.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6401b596",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wiki['author'] = range(df_wiki.shape[0])\n",
    "df_wiki['author'] = df_wiki['author'].apply(lambda x: 'a' + str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff01f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wiki['clean_text'] = df_wiki['abstract_original'].apply(lambda x: reddit_preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dee792",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wiki.drop(['abstract_original'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474547c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned .csv\n",
    "df_wiki.to_csv('data/wikipedia_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70885f29",
   "metadata": {},
   "source": [
    "# Enron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8404cec-8604-4666-a85f-790c751e82e9",
   "metadata": {},
   "source": [
    "The Enron dataset was made available in the Kaggle platform.\n",
    "\n",
    "To download the dataset, please go to [this link](https://www.kaggle.com/wcukierski/enron-email-dataset) and click on the button **Download** in the upper right corner.\n",
    "\n",
    "After that, unzip the content and put the file `emails.csv` in this project's `data` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc6c0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_enron = pd.read_csv(\"data/emails.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab9e774",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_enron['author'] = df_enron['file'].str.split('/').str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126a826f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def body_clean(mess):\n",
    "    '''Function to extract body/message of e-mail'''\n",
    "    e = email.message_from_string(mess)\n",
    "    e2 = e.get_payload()\n",
    "    return reddit_preprocess(e2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc221ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_enron['clean_text'] = df_enron['message'].apply(lambda x: body_clean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3efa7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_enron.drop(['file','message'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f35e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned .csv\n",
    "df_enron.to_csv('data/enron_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0ee68c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
